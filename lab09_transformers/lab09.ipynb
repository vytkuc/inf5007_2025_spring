{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF5007 Neural Networks / Neuroniniai tinklai\n",
    "**LAB9**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK TASK\n",
    "\n",
    "You can use transformers in your individual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Hugging Face API\n",
    "\n",
    "Full course on how to use the API: https://huggingface.co/course/chapter0/1?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hugging face API \n",
    "The Hugging Face API was created as a space to train, load and share large deep learning NLP models. Several of the possible tasks that you can solve using the models provided in the library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "Model: https://huggingface.co/assemblyai/distilbert-base-uncased-sst2?text=I+like+you.+I+love+you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "Model: https://huggingface.co/microsoft/DialoGPT-small?text=Hey+my+name+is+Mariama%21+How+are+you%3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm back!\n",
      "DialoGPT: Hi back\n",
      "DialoGPT: Hi back\n",
      "DialoGPT: Hi water\n",
      "DialoGPT: I like turtles\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "for step in range(5):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked learning\n",
    "Model: https://huggingface.co/bert-base-multilingual-cased?text=I+like+%5BMASK%5D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03385859727859497,\n",
       "  'token': 10829,\n",
       "  'token_str': 'London',\n",
       "  'sequence': 'I live in London'},\n",
       " {'score': 0.03125724568963051,\n",
       "  'token': 11619,\n",
       "  'token_str': 'Italy',\n",
       "  'sequence': 'I live in Italy'},\n",
       " {'score': 0.023685377091169357,\n",
       "  'token': 12775,\n",
       "  'token_str': 'Germany',\n",
       "  'sequence': 'I live in Germany'},\n",
       " {'score': 0.020019808784127235,\n",
       "  'token': 12962,\n",
       "  'token_str': 'live',\n",
       "  'sequence': 'I live in live'},\n",
       " {'score': 0.016861874610185623,\n",
       "  'token': 18744,\n",
       "  'token_str': 'Moscow',\n",
       "  'sequence': 'I live in Moscow'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
    "unmasker(\"I live in [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3543,  0.0795,  0.4237,  ...,  0.7303,  0.2264,  0.1486],\n",
      "         [-0.3508, -0.4988,  0.2034,  ...,  0.0907, -0.0760, -0.4666],\n",
      "         [-0.2361, -0.4036, -0.0100,  ...,  0.4503, -0.2976,  0.1432],\n",
      "         ...,\n",
      "         [-0.1339, -0.1011, -0.1279,  ...,  0.5724, -0.2574, -0.2440],\n",
      "         [ 0.1079,  0.1582, -0.0425,  ...,  0.8293,  0.1642, -0.0081],\n",
      "         [ 0.3190,  0.0924,  0.5734,  ...,  0.2694,  0.3159,  0.0590]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 4.4070e-01, -2.4586e-01,  2.5098e-01, -3.7415e-01, -1.4130e-01,\n",
      "          4.9311e-01,  3.5765e-01,  2.8654e-01, -5.3179e-01,  2.5917e-01,\n",
      "         -2.0071e-01, -2.9737e-01, -2.2273e-01, -5.7293e-02,  1.8640e-01,\n",
      "         -2.4702e-01,  7.8171e-01,  2.8670e-01,  3.2563e-01, -2.7157e-01,\n",
      "         -9.9564e-01, -2.7828e-01, -5.3223e-01, -3.3896e-01, -4.7615e-01,\n",
      "          6.3296e-02, -1.7648e-01,  2.6515e-01,  4.7409e-01, -3.2620e-01,\n",
      "          1.4310e-01, -9.9633e-01,  8.6469e-01,  8.0995e-01,  2.6563e-01,\n",
      "         -3.0641e-01,  2.2481e-01,  2.6704e-01,  4.5788e-01, -3.5471e-01,\n",
      "         -1.5926e-02, -8.8114e-04, -2.4452e-02,  9.0294e-02, -1.5559e-01,\n",
      "         -3.6206e-01, -2.9142e-01,  3.5288e-01, -3.9109e-01,  1.6348e-01,\n",
      "          1.3456e-01,  8.8279e-02,  5.9134e-01,  4.2545e-01,  4.8279e-01,\n",
      "          3.1098e-01,  2.0549e-01,  1.7004e-01,  4.2687e-01, -5.5494e-01,\n",
      "         -1.8930e-01,  4.5859e-01,  1.6088e-01, -2.2832e-02, -4.6040e-01,\n",
      "         -1.7955e-01,  2.0466e-01, -2.2681e-01,  6.0300e-01, -3.2594e-01,\n",
      "         -3.9285e-01, -3.0058e-01, -2.9319e-01,  2.7982e-01,  1.2123e-01,\n",
      "         -3.2005e-01,  2.5368e-01,  1.8936e-01,  2.4802e-01, -1.7828e-01,\n",
      "         -5.5956e-01, -5.4082e-01, -5.0746e-01,  2.9236e-01, -1.1518e-01,\n",
      "          4.6860e-01,  4.1968e-01, -5.7075e-01,  3.1453e-01, -1.0026e-01,\n",
      "          3.8190e-01,  6.1652e-01, -2.3903e-01,  2.2304e-01, -3.2372e-02,\n",
      "         -2.4309e-01, -8.3304e-01, -3.8787e-01, -2.2530e-01, -4.9311e-01,\n",
      "         -3.2063e-01,  3.8827e-01, -4.7285e-01, -8.2521e-02, -4.0683e-01,\n",
      "         -4.6778e-01,  1.7195e-01,  3.9528e-01, -1.2958e-01,  2.4121e-01,\n",
      "          3.3582e-01, -4.7267e-01, -2.3799e-01,  1.1128e-01, -4.2894e-01,\n",
      "          9.9003e-01, -6.3555e-01,  4.8444e-01,  2.2495e-01, -1.2318e-01,\n",
      "         -5.3284e-01,  9.9712e-01,  3.4736e-01, -3.6382e-01,  6.1575e-02,\n",
      "          3.5973e-01, -5.6858e-01,  2.2218e-01,  5.2305e-01,  4.1848e-01,\n",
      "         -4.8312e-02, -1.5490e-01, -2.6627e-01, -5.3674e-01, -9.1918e-01,\n",
      "         -2.6869e-01, -4.3830e-01,  3.8187e-01, -2.3240e-01, -1.2910e-01,\n",
      "          2.4706e-01,  5.4893e-01,  3.2231e-01, -7.9563e-02, -1.5950e-01,\n",
      "         -2.9013e-01,  4.5308e-01, -3.4065e-01,  9.9612e-01,  8.0251e-01,\n",
      "         -3.3366e-01, -2.6681e-01,  8.4152e-01, -6.5881e-01, -2.3516e-01,\n",
      "         -2.9595e-01, -4.1709e-01, -5.4035e-01,  1.5270e-01,  1.9488e-01,\n",
      "          2.8196e-01, -2.8367e-01, -3.6664e-01, -2.9263e-01,  2.0248e-01,\n",
      "         -6.5233e-01, -5.3172e-02,  4.7386e-01,  4.5377e-01,  2.4238e-01,\n",
      "         -3.4451e-01,  4.7612e-01,  4.0383e-02, -2.3867e-01, -1.8463e-01,\n",
      "          4.1152e-01,  5.1596e-01, -5.2791e-02, -2.9694e-01, -1.3272e-01,\n",
      "          2.2177e-01, -4.5222e-01, -6.2216e-01,  3.1068e-01, -3.0166e-01,\n",
      "         -3.1798e-01,  2.7527e-01,  1.7579e-02, -2.9865e-01,  3.6767e-01,\n",
      "         -3.3411e-01,  1.4592e-01, -3.6549e-01,  3.7184e-01,  3.5253e-01,\n",
      "          6.4999e-02, -4.1033e-01,  1.4743e-01,  3.2379e-01,  3.5487e-01,\n",
      "          3.4379e-01,  1.4774e-01,  2.7034e-01,  4.4191e-01, -5.1663e-01,\n",
      "         -9.2050e-01,  4.6997e-01,  2.3027e-01,  6.7503e-01, -3.9627e-01,\n",
      "         -3.9553e-01, -3.2287e-01,  5.7350e-01,  2.8002e-01, -1.5338e-01,\n",
      "          2.7642e-01,  6.0353e-01, -4.8992e-01, -3.6013e-01,  3.3765e-01,\n",
      "          8.1283e-02, -4.8598e-01, -1.8419e-01, -5.9565e-01, -2.1754e-01,\n",
      "          4.1763e-01,  1.8128e-01,  1.7993e-01,  8.7790e-02, -2.3883e-01,\n",
      "         -2.6513e-01, -2.0868e-01,  2.0155e-01,  6.1485e-01, -2.0598e-01,\n",
      "          8.7120e-01, -2.7829e-01,  3.2392e-01, -5.3130e-01, -4.5939e-01,\n",
      "          3.8472e-01,  1.7750e-02,  3.0465e-01,  9.6346e-01,  4.3203e-01,\n",
      "         -5.2690e-01,  3.7504e-01,  1.6364e-01, -9.9324e-02, -1.1992e-01,\n",
      "          2.4057e-01, -5.9197e-01,  9.1513e-01,  3.7099e-01,  3.7097e-01,\n",
      "         -9.9614e-01,  3.8204e-01,  2.5641e-01,  4.1841e-01,  2.1443e-01,\n",
      "          4.1517e-01,  1.2579e-01,  4.7124e-01,  9.6370e-01, -4.2910e-01,\n",
      "         -5.8140e-01, -5.1062e-01, -1.5957e-01, -3.8929e-01, -4.2930e-01,\n",
      "         -4.4941e-01, -2.2222e-01, -1.9629e-01, -4.7690e-01, -1.1273e-01,\n",
      "          4.5337e-01,  4.7729e-01, -9.9784e-01,  9.3260e-01,  3.0026e-01,\n",
      "         -3.1836e-01, -1.1830e-01,  4.7377e-01, -9.9600e-01,  8.9707e-02,\n",
      "         -1.6958e-01, -4.4473e-01,  9.6282e-02, -6.5824e-01, -3.1445e-01,\n",
      "          3.8897e-01,  3.0760e-01,  4.3926e-01, -4.5738e-02,  1.3938e-01,\n",
      "          5.1543e-01, -1.4554e-01, -3.7697e-02, -1.1264e-02, -2.8085e-01,\n",
      "          7.6764e-01,  1.9848e-02,  2.1437e-01,  3.4473e-01, -1.2146e-01,\n",
      "          4.5215e-01, -3.2594e-01,  5.4550e-01,  1.6767e-01,  1.8356e-01,\n",
      "          1.1311e-02, -3.0569e-01,  5.1024e-01, -8.0188e-01,  2.9277e-01,\n",
      "         -2.3554e-01, -4.2702e-01,  1.8502e-01,  1.9935e-01, -1.7421e-01,\n",
      "         -2.5867e-01,  2.4611e-01, -5.1870e-01,  9.9655e-01,  3.8300e-01,\n",
      "         -4.2240e-01, -6.2099e-01,  5.9395e-01,  7.7790e-01, -1.9050e-01,\n",
      "         -9.5793e-01, -2.8280e-01,  6.9796e-01,  3.6524e-01,  3.8538e-01,\n",
      "          2.8604e-01,  2.3082e-01,  2.2017e-01, -1.5468e-01, -6.2403e-02,\n",
      "          1.3766e-01, -6.2859e-01,  4.2872e-01, -4.8924e-01, -6.5724e-01,\n",
      "          1.6771e-01, -2.3100e-01, -3.3329e-01, -9.5150e-01,  3.3801e-01,\n",
      "          4.3061e-01,  3.9420e-01,  4.7242e-01,  3.1421e-01, -2.4470e-01,\n",
      "          5.3194e-01,  4.2278e-01, -5.2344e-01, -2.5453e-01, -2.8110e-01,\n",
      "         -1.1408e-01,  1.0633e-01, -2.6994e-01, -5.0214e-01,  1.3769e-01,\n",
      "         -7.7194e-01,  1.1579e-01,  1.0998e-01, -4.4316e-01, -5.4377e-01,\n",
      "          2.5335e-01, -9.9714e-01, -6.5268e-03,  1.4122e-01, -3.0597e-01,\n",
      "          3.2768e-01, -4.2944e-01, -2.6694e-01,  2.4774e-01,  1.7591e-01,\n",
      "         -1.4138e-01,  3.0633e-01, -6.3263e-01,  2.7250e-01, -1.9154e-01,\n",
      "          1.3559e-01,  8.4605e-01,  6.0932e-01,  2.2239e-01, -1.6245e-01,\n",
      "          3.8561e-01, -3.3523e-01, -4.1081e-01,  2.9944e-01,  5.6236e-01,\n",
      "         -1.2660e-01,  1.4603e-01,  1.4437e-01,  8.5209e-02, -3.0615e-01,\n",
      "          4.8288e-01, -2.6102e-01, -5.0903e-02,  3.4574e-01,  3.3832e-02,\n",
      "          8.4037e-03, -6.1096e-01,  3.0773e-01, -6.0942e-01,  4.3712e-01,\n",
      "          4.5354e-02,  2.8751e-01,  1.5830e-01,  4.2438e-01, -4.8978e-01,\n",
      "         -2.8359e-01,  1.8874e-01, -2.1446e-01, -4.6549e-01, -1.9099e-01,\n",
      "         -3.5883e-01,  9.9683e-01,  5.1633e-01,  3.0726e-01, -2.0402e-01,\n",
      "          2.3233e-02,  4.3620e-01, -2.0508e-01,  4.8394e-01,  4.5364e-01,\n",
      "          3.2361e-01, -8.8701e-02,  1.1295e-01,  5.5227e-01,  1.8644e-01,\n",
      "          2.1619e-01,  5.7071e-01,  6.5495e-01, -4.7590e-01,  7.2256e-01,\n",
      "         -3.7413e-01, -2.7425e-01, -9.9784e-01,  3.2629e-01,  6.1771e-01,\n",
      "         -3.5262e-01, -8.7407e-01,  2.0033e-01, -3.4450e-01,  2.2732e-01,\n",
      "         -2.8337e-01,  1.4363e-01,  7.8696e-02, -1.5683e-01,  6.0291e-01,\n",
      "         -3.2666e-01,  9.9669e-01, -3.7058e-01,  3.3573e-02,  4.3806e-01,\n",
      "          2.0576e-01, -3.7645e-01, -3.0420e-01, -4.1597e-01,  2.0363e-01,\n",
      "         -1.0669e-01,  3.4435e-01, -9.5377e-01,  7.7963e-02,  3.8638e-02,\n",
      "          3.6665e-01, -3.8164e-01,  1.7315e-01, -5.2631e-01,  2.9537e-01,\n",
      "         -2.5767e-01, -1.8463e-01, -2.8108e-01,  3.9637e-01, -2.8925e-01,\n",
      "          5.4740e-01, -1.9857e-01,  5.9137e-02, -4.4752e-01,  4.1554e-01,\n",
      "         -2.0115e-01,  2.9539e-01, -3.5616e-02,  7.0702e-02, -3.5333e-01,\n",
      "         -2.9628e-01, -3.1411e-01,  3.3264e-01, -2.0266e-01,  9.9578e-01,\n",
      "         -2.3428e-01,  3.7388e-01, -3.3261e-01,  4.5377e-01, -4.0671e-01,\n",
      "          6.0917e-01,  7.6212e-01, -1.3887e-01,  3.0111e-01,  4.9484e-01,\n",
      "         -7.9257e-01,  5.5942e-01, -2.4844e-01, -9.6948e-01, -2.6213e-01,\n",
      "          9.8382e-01,  4.2112e-02,  5.3035e-01,  6.3033e-01,  5.6310e-01,\n",
      "          5.2975e-01, -4.4125e-01,  3.8382e-01,  8.4885e-01,  1.1127e-01,\n",
      "          2.9343e-01,  3.3433e-01,  1.6681e-01, -5.1252e-01, -3.2900e-01,\n",
      "          9.9649e-01,  9.9637e-01,  1.4312e-01,  3.6280e-01, -4.5103e-01,\n",
      "         -4.9175e-01, -2.1636e-01,  3.2290e-01,  7.3154e-02,  4.6206e-01,\n",
      "         -3.6897e-02,  2.0115e-01, -4.2542e-01, -2.5790e-01, -2.2660e-01,\n",
      "         -3.6272e-01, -4.6727e-01,  1.8149e-01, -4.9601e-01,  6.6848e-01,\n",
      "          5.5687e-01,  2.3704e-01,  5.9336e-01,  2.9603e-01,  3.6686e-01,\n",
      "         -1.4166e-01, -3.6497e-01,  4.2736e-01, -4.2580e-01, -1.8430e-01,\n",
      "         -2.9939e-01,  4.2925e-01, -9.9556e-01, -1.9117e-01, -2.1745e-01,\n",
      "         -3.0705e-01,  6.1007e-01,  2.9619e-01,  4.5087e-01, -3.2857e-01,\n",
      "         -2.1157e-01, -2.9103e-01,  1.4673e-01,  3.6212e-01,  2.9557e-01,\n",
      "         -2.0796e-01, -3.8047e-01,  3.9608e-01, -5.8376e-01,  2.3368e-01,\n",
      "         -2.8689e-01, -2.8764e-01, -7.8399e-01, -3.5756e-01, -3.2304e-01,\n",
      "          4.8467e-01, -4.5606e-01, -4.5276e-01,  2.5626e-01,  2.6387e-01,\n",
      "          2.9675e-01, -5.1157e-01,  1.8882e-01, -2.2940e-01,  2.9039e-02,\n",
      "          3.9774e-01,  3.0077e-01,  3.7527e-01, -3.5700e-01, -2.7118e-01,\n",
      "         -3.1361e-01, -4.0172e-01, -1.6731e-01,  2.5676e-01, -1.9095e-01,\n",
      "          3.1246e-01, -4.4956e-02,  3.1095e-01, -2.8861e-01,  2.8042e-01,\n",
      "          1.7922e-01,  5.4044e-01, -3.9216e-01,  7.1314e-01,  5.8614e-01,\n",
      "         -2.3799e-01,  2.9738e-01,  3.1647e-01, -4.8674e-01, -1.1187e-01,\n",
      "          9.9665e-01,  6.4811e-01,  2.2023e-01,  3.6741e-01, -2.5400e-01,\n",
      "          3.4355e-01,  1.4512e-01,  5.9382e-01, -3.4263e-01,  9.8364e-01,\n",
      "         -2.5089e-01,  3.2558e-01,  4.1004e-01,  5.4063e-01,  2.6803e-01,\n",
      "          2.6771e-01,  3.5801e-01,  8.2005e-01,  1.8085e-01,  2.6572e-01,\n",
      "          3.1694e-01,  3.2217e-01,  4.6155e-01,  3.0364e-01,  2.7967e-02,\n",
      "          5.0248e-01,  3.2496e-01, -4.9333e-01,  4.4109e-01, -4.7429e-01,\n",
      "         -2.0677e-01, -2.0755e-01, -7.8564e-02, -3.2627e-01,  1.4529e-01,\n",
      "         -2.5811e-01, -1.1544e-01, -1.7574e-01,  2.6787e-01, -1.9094e-01,\n",
      "          5.3237e-02, -8.0530e-02, -4.1851e-01,  5.7382e-01, -4.2610e-01,\n",
      "          4.3120e-01, -3.1496e-01,  7.4919e-02, -9.0904e-01,  1.1975e-01,\n",
      "         -7.9564e-02, -3.4482e-01, -4.9242e-01, -6.3276e-01,  4.3130e-01,\n",
      "          4.4975e-01, -3.5823e-01,  4.7882e-01, -4.7281e-01,  3.8178e-01,\n",
      "         -4.6108e-01, -3.3525e-01,  1.8811e-01, -9.9593e-01,  1.2170e-01,\n",
      "          2.7528e-01, -3.6393e-01,  1.3208e-01,  1.4229e-01,  3.5747e-01,\n",
      "          3.8370e-01, -4.7202e-01, -4.7193e-01, -3.0128e-01,  5.1687e-01,\n",
      "         -1.6260e-01,  3.4768e-01,  3.3077e-01, -4.7770e-01, -2.6212e-01,\n",
      "          2.3692e-01, -3.6086e-01,  4.0658e-01,  2.1177e-01, -1.4885e-01,\n",
      "          4.7037e-01, -3.0447e-01,  4.8153e-02, -4.6069e-01,  4.7578e-01,\n",
      "         -4.0378e-01, -3.9128e-01,  4.3990e-01, -4.0089e-01, -4.7403e-01,\n",
      "         -1.1667e-01,  3.5912e-01, -1.5668e-01,  2.6616e-01,  3.0053e-01,\n",
      "         -1.3587e-01,  2.3051e-01, -5.3906e-01,  3.0004e-01, -1.9913e-01,\n",
      "          3.3574e-01, -8.7017e-01, -4.2372e-01, -6.0909e-01, -3.9277e-01,\n",
      "          2.4852e-01,  4.2407e-01,  3.3510e-01,  3.4699e-01, -6.6198e-02,\n",
      "          8.0038e-02, -1.8129e-01,  4.9702e-01,  4.0528e-01, -1.7412e-01,\n",
      "          1.1640e-02, -3.1969e-01,  2.6660e-01, -4.9996e-01, -3.6763e-02,\n",
      "         -9.9636e-01, -3.8855e-01,  2.8816e-01,  4.0824e-01,  4.1663e-01,\n",
      "         -3.2664e-01, -2.0754e-01, -4.8751e-01, -1.3978e-01,  4.0474e-01,\n",
      "          4.8106e-01,  6.0150e-01,  4.7923e-01,  3.4330e-01, -1.7023e-01,\n",
      "         -7.2539e-02,  9.3396e-01, -4.2611e-01,  1.1441e-01,  6.2738e-01,\n",
      "          1.3930e-01,  9.7804e-01,  2.7644e-01,  2.8737e-01,  2.7732e-01,\n",
      "         -4.2842e-01,  4.5034e-01,  4.5722e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text = \"This is the text I'd like to train my deep learning model with\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning your own model\n",
    "\n",
    "We will use IMDB movie reviews dataset to finetune a DistilBERT model for sentiment analysis. The dataset can be found here: https://huggingface.co/datasets/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires a lot of computing resources\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 4.31kB [00:00, 2.66MB/s]                   \n",
      "Downloading: 2.17kB [00:00, 1.49MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /home/milita/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 84.1M/84.1M [02:07<00:00, 659kB/s] \n",
      "                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/milita/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1524.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 20.9kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 547kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 369kB/s] \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 503kB/s]  \n",
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.82ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [06:58<00:00, 640kB/s]  \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"finetuning-sentiment-model-3000-samples\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "/home/milita/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 376\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmilasong\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/milasong/huggingface/runs/fglzsgch\" target=\"_blank\">finetuning-sentiment-model-3000-samples</a></strong> to <a href=\"https://wandb.ai/milasong/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 187/376 [00:28<00:30,  6.29it/s]Saving model checkpoint to finetuning-sentiment-model-3000-samples/checkpoint-188\n",
      "Configuration saved in finetuning-sentiment-model-3000-samples/checkpoint-188/config.json\n",
      "Model weights saved in finetuning-sentiment-model-3000-samples/checkpoint-188/pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples/checkpoint-188/tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples/checkpoint-188/special_tokens_map.json\n",
      "100%|█████████▉| 375/376 [00:58<00:00,  6.43it/s]Saving model checkpoint to finetuning-sentiment-model-3000-samples/checkpoint-376\n",
      "Configuration saved in finetuning-sentiment-model-3000-samples/checkpoint-376/config.json\n",
      "Model weights saved in finetuning-sentiment-model-3000-samples/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples/checkpoint-376/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 376/376 [00:59<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 62.3944, 'train_samples_per_second': 96.163, 'train_steps_per_second': 6.026, 'train_loss': 0.29486384290329953, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=376, training_loss=0.29486384290329953, metrics={'train_runtime': 62.3944, 'train_samples_per_second': 96.163, 'train_steps_per_second': 6.026, 'train_loss': 0.29486384290329953, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 16\n",
      "100%|██████████| 19/19 [00:03<00:00,  5.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3351273238658905,\n",
       " 'eval_accuracy': 0.87,\n",
       " 'eval_f1': 0.8729641693811074,\n",
       " 'eval_runtime': 3.4058,\n",
       " 'eval_samples_per_second': 88.085,\n",
       " 'eval_steps_per_second': 5.579,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
